# 참고 : github.com/hunkim/DeepLearningZeroToAll

1. linear regression의 cost function = 통계에서의 SSR
2. cost function => 제곱의 의미 : 차이가 클 때 더 큰 패널티를 줌 : (f(x) - y)^2
3. cost function = 평균제곱오차
4. constanct(상수) 메소드로 '노드' 생성
5. 텐서 실행은 Session 생성 후 노드 그래프를 실행시키는 방식으로 진행됨
6. 그래프(노드) 빌드(정의) -> 세션 run -> 결과 return
7. constant : 고정된 숫자 노드 / placeholder : 미지수를 갖는 노드
8. placeholder -> 세션 run(실행식 변수, feed_dict) -> 미지수를 feed_dict으로 먹여줌
9. Rank : 스칼라 -> 벡터 -> 매트릭스 -> 3텐서 -> n텐서
10. tf.Variable 노드로 가중치(W)와 bias(b) 선언
11. reduce_mean : 평균 내줌(cost의 1/n * 시그마 부분)
12. tf.Variable(W,b) 실행시키기 전엔 tf,global_variables_initializer() 실행시켜줘야함
13. hypothesis(x*W + b), cost(W,b) 그래프 생성 -> X와 Y 값을 feed_dict -> W,b update & value return해서 확인
14. 다항식(=독립변수가 n개/가중치도 n개 = multivariable)을 이용한 hypothesis (X*W)는 매트릭스(행렬 곱)를 통해 표현할 수 있음 : H(X) = XW
15. 수많은 행(instance)이 존재하더라도 매트릭스(행렬곱)을 이용하면 수식 동일함 : H(X) = XW
16. X : [n, 3]의 구조를 가질 때, numpy에서는 n개를 -1로 표현하고, tensorflow에서는 None으로 표시함
17. cost(W)는  H(x)를 이용하여 cost 함수 값이 최소가 되는 W값을 구하는 함수로, W를 구하기 위해 cost(W) 함수를 미분하면 W = W - 알파*(W에 대해 미분한 cost(W)) 가 되는데 여기서 알파값은 learning_rate임
18. Linear Regression 형태의 H(x) =Wx가 다양한 값을 뱉어내기 때문에 분류(0,1) 문제에서 적합하지 않았음 -> 이를 위해 g(z) = 1/(1+e^-z) 함수가 탄생하게 되었고, 이를 시그모이드라고 부르기도 하고, logistic이라고 부르기도 함
(각각의 hat(y)에 시그모이드를 적용함)
19. 시그모이드 함수를 이용해서 값을 0~1사이로 압축해줌
20 .H(x) = hat(y) = y 예측값
21. 소프트맥스 함수 : 어떤 예측값(hat(Y) = 20)이 각 예측값(a or b or c)일 확률을 각각 나타내주고, 각 확률의 합이 1이 되도록 만들어줌
22. 소프트맥스를 통과한 결과(0.7 / 0.2 / 0.1)에 대해 one-hot encoding을 해주면(=argmax함수 사용) 0.7만 1의 값이고 0.7을 제외한 결과값은 0이 되어 a로 분류됨을 알 수 있음
23. logits(로짓) = tf.matmul(X,W) + b
- hypothesis = tf.nn.softmax(logits)
- hypothesis : 확률값
24. 다층 퍼셉트론(XOR) : 병렬형식의 단층 네트워크(유닛)를 하나의 네트워크(유닛)으로 결합시킨 후 다음 네트워크(유닛)의 X자리에 대입함
K(X) = sigmoid( X*W1+B1 )
hat(Y) = H(X) = sigmoid( K(X)*W2 + B2 )
=> K = tf.sigmoid(tf.matmul(X,W1) + B1)
hypothesis = tf.sigmoid(tf.matmul(K, W2) + B2)
25. Partial derivative : 관심있는 항목 외엔 상수로 보고 미분하는 방법
26. chain rule : 복합함수 미분 : x가 최종적으로 f에 미치는 영향을 구하기 위함 : f(g(x)) 미분 = df/dx = (df/dg * dg/dx)
27. y 결과값이 0 또는 1일 땐 복잡한 soft_max를 사용할 필요없이 logistic regression을 사용해줌
28. TensorBoard : 학습을 길게 할 때, 학습의 진행사항을 볼 수 있게 해주는 도구
step에 따른 cost 값 변화를 그래프로 보여줌
29. 시그모이드를 사용한 딥네트워크의 경우, 1단에 비해 2~3단의 딥네트워크가 더 잘 작동하지만, 2~3단이 아닌 10단이 되었는데도 결과가 좋지 않은 경우가 있음
이 경우는 오차역전파법을 통해 생각해보면 되는데, 어떠한 값을 미분 해내려가다보면 중간중간에서 시그모이드 함수를 만나게됨. 이 시그모이드 함수는 해당 값을 0~1 사이의 값으로 변환시켜주게됨, 대부분 0은 아니지만 0에 가까운 값(ex. 0.01)으로 변환되기 때문에 chain rule에 의해 0.01 * 0.01 * 0.01 ... 이 되다보면 아주 작은값이 되버림
즉, 최종 출력값(Y)에서 2~3단까지는 값이 괜찮지만 네트워크가 깊어질수록 값이 아주 작아지기 때문에 최종 미분값은 0에 가까운 값이 되어버리고 해당 미분값이 출력값(Y)에 영향을 미치지 않는다는 것과 같은 말이 됨
=> 기울기(경사도)가 사라짐 = 학습이 안됨 = Vanishing gradient
=> 해결법 : sigmoid 대신 ReLU(또는 Leaky ReLU) 사용 : 0보다 작으면 0, 0보다 크면 값에 비례해서 증가
=> 더 이상 시그모이드 사용X, ReLU 사용O, 하지만 출력값은 0~1 사이의 값으로 나와야하므로 학습 마지막 단은 시그모이드 사용
(종류 : maxout / ReLU / VLReLU / tanh / Sigmoid)
30. 최적의 Weight 값 구하는 방법 :
Forward의 X * W = Y 값과 Backward의 Y = W * X의 값을 구해서, Forward의 X값과 Backward의 hat(X)값 간의 차이가 가장 작아지는 W 값을 구하면 됨 : RBM : Xavier initialization
=> Forward(=encoder) / Backward(=decoder) 라고도 함
* RBM 쓴 것과 비슷하게 W 값을 잘 설정하는 방법 : 
# Glorot et al. 2010
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
# He et al. 2015
W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in/2)
31. Overfitting 줄이는 방법 :
(1) More training data : 많은 학습데이터
(2) Reduce the number of features : 속성 수 줄이기(딥러닝에서는 안해도 됨)
(3) Regularization : 정규화
- cost + learning_rate * sigma(W^2)
: learning_rate 값에 따라 중요도 결정됨(0.01 : 어느정도 중요 / 0.1 : 중요)
: sigma(W^2) 값이 최소가 되는 W 값 결정
: L2reg = 0.001 * tf.reduce_sum(tf.square(W)) :  
(4) Dropout : 랜덤하게 노드를 끊어버림 : 학습할떄만 drop out 해줌 : dropout_rate 값 설정
# Train
sess.run(optimizer, feed_dict = {X : batch_xs, Y : batch_ys, dropout_rate : 0.7})
# EVALUATION :
print "Accuracy : ", accuracy.eval({X : mnist.test.images, Y : mnist.test.labels, dropout_rate : 1})
32. Optimizers
- 이전엔 GradientDescentOptimizer를 사용함
train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)
- 하지만 Optimizers에는 여러 종류가 있음
(1) tf.train.AdadeltaOptimizer
(2) tf.train.AdagradOptimizer
(3) tf.train.AdagradDAOptimizer
(4) tf.train.MomentumOptimizer
(5) tf.train.AdamOptimizer
(6) tf.train.FtrlOptimizer
(7) tf.train.ProximalGradientDescentOptimizer
(8) tf.train.ProximalAdagradOptimizer
(9) tf.train.RMSPropOptimizer
- 위 결과에 대한 시뮬레이션 : http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html
- 하지만 통상적으로 Adam Optimizer가 가장 나음
# GradientDescentOptimizer를 AdamOptimizer으로만 바꿔주면 됨
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = hypothesis, labels = Y))
optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)
33. CNN (Convolution layer)
- 필터 : 전체 이미지의 한번에 받지 않고, 일부만 가져와서 받아오는 것
- 32*32*3 image와 5*5*3 filter의 마지막 값(색깔 : 3)은 동일함
- 필터의 크기(5*5)는 본인이 정의할 수 있음 : 한번에 얼마만큼 보고싶은지?
- 필터는 하나의 값을 만들어냄 : Wx + b 식 이용
- 7*7 input에서 (3*3 filter + stride : 1)을 사용하면, 5*5 output(image)이 나옴
- 7*7 input에서 (3*3 filter + stride : 2)을 사용하면, 3*3 output(image)이 나옴
- N * N image에서 필터 사이즈가 F * F 일 때, 몇개의 Output size로 출력될 것인가?
* Output size =  (N - F) / stride + 1
(N : 이미지의 가로세로 길이, F : 필터의 가로세로 길이, stride : 옆으로 몇칸 움직일 것인지?)
=> 필터를 이용해서 output을 생성하면 이미지의 크기가 작아짐 = 정보를 잃어버림(= 합성곱 네트워크를 사용할 때는 pad를 사용함)
34. pad : image의 테두리를 0으로 두름
* pad 사용 이유 :
(1) 그림이 급격하게 작아지는 것 방지
(2) 모서리 부분을 네트워크에 알려주는 역할
=> 7*7 image에 pad 적용 -> 9*9
9*9 image에 (3*3 filter, stride : 1) 적용 : 식에 의해 9-3/1 +1 = 6 + 1 = 7로 7*7 image가 output으로 나옴
즉, 7*7 image에 pad를 적용한 후 (3*3 filter, stride : 1) 적용하면 그대로 7*7 image가 output 값이 됨
35. 32*32*3 image -> 5*5*3인 6개의 filter -> 이 6개의 filter는 각각의 W이 다르기 때문에 각각 차이가 있을 것이고, 이 6개의 filter를 Convolution Layer로 합치게 되면 activation maps이 됨
이 activation maps은 pad를 적용하지 않았다는 가정하에 (28, 28, 6)이 됨
=> Output size : (32-5)/1 + 1 = 27/1 + 1 = 28
     filter 개수 = 6
즉, 정리하자면 image에 filter 처리해서 conv layer를 만든 것, 깊이는 몇개의 필터를 사용하느냐에 따라 달라짐
36. Pooling layer : Sampling이라고 보면 됨
Max Pooling : 각 filter에서 가장 큰 값을 추출함
Max Pooling한 값을 모아서 새로운 output을 만듦
(이때, output의 사이즈는 filter의 크기(n*n)와 stride 값에 따라 달라짐)
37. Fully Connected Layer (FC layer)의 구성 예시 :
* CONV - RELU - CONV - RELU - POOL - CONV - RELU - CONV - RELU - POOL - CONV - RELU - CONV - RELU - POOL
* 마지막은 POOL로 마무리함
* output 결과가 3*3*10이라면 깊이가 10이라는 뜻
38. Fully Connected Layer(FC layer) 어떻게 동작하는지 & 각각의 layer가 어떻게 동작하는지 확인 : http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html
39. CONV - POOL - NORM(Normalization layer)
: NORM은 굳이 안해줘도 됨 / 하나 안하나 비슷함







