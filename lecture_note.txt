# 참고 : github.com/hunkim/DeepLearningZeroToAll

1. linear regression의 cost function = 통계에서의 SSR
2. cost function => 제곱의 의미 : 차이가 클 때 더 큰 패널티를 줌 : (f(x) - y)^2
3. cost function = 평균제곱오차
4. constanct(상수) 메소드로 '노드' 생성
5. 텐서 실행은 Session 생성 후 노드 그래프를 실행시키는 방식으로 진행됨
6. 그래프(노드) 빌드(정의) -> 세션 run -> 결과 return
7. constant : 고정된 숫자 노드 / placeholder : 미지수를 갖는 노드
8. placeholder -> 세션 run(실행식 변수, feed_dict) -> 미지수를 feed_dict으로 먹여줌
9. Rank : 스칼라 -> 벡터 -> 매트릭스 -> 3텐서 -> n텐서
10. tf.Variable 노드로 가중치(W)와 bias(b) 선언
11. reduce_mean : 평균 내줌(cost의 1/n * 시그마 부분)
12. tf.Variable(W,b) 실행시키기 전엔 tf,global_variables_initializer() 실행시켜줘야함
13. hypothesis(x*W + b), cost(W,b) 그래프 생성 -> X와 Y 값을 feed_dict -> W,b update & value return해서 확인
14. 다항식(=독립변수가 n개/가중치도 n개 = multivariable)을 이용한 hypothesis (X*W)는 매트릭스(행렬 곱)를 통해 표현할 수 있음 : H(X) = XW
15. 수많은 행(instance)이 존재하더라도 매트릭스(행렬곱)을 이용하면 수식 동일함 : H(X) = XW
16. X : [n, 3]의 구조를 가질 때, numpy에서는 n개를 -1로 표현하고, tensorflow에서는 None으로 표시함
17. cost(W)는  H(x)를 이용하여 cost 함수 값이 최소가 되는 W값을 구하는 함수로, W를 구하기 위해 cost(W) 함수를 미분하면 W = W - 알파*(W에 대해 미분한 cost(W)) 가 되는데 여기서 알파값은 learning_rate임
18. Linear Regression 형태의 H(x) =Wx가 다양한 값을 뱉어내기 때문에 분류(0,1) 문제에서 적합하지 않았음 -> 이를 위해 g(z) = 1/(1+e^-z) 함수가 탄생하게 되었고, 이를 시그모이드라고 부르기도 하고, logistic이라고 부르기도 함
(각각의 hat(y)에 시그모이드를 적용함)
19. 시그모이드 함수를 이용해서 값을 0~1사이로 압축해줌
20 .H(x) = hat(y) = y 예측값
21. 소프트맥스 함수 : 어떤 예측값(hat(Y) = 20)이 각 예측값(a or b or c)일 확률을 각각 나타내주고, 각 확률의 합이 1이 되도록 만들어줌
22. 소프트맥스를 통과한 결과(0.7 / 0.2 / 0.1)에 대해 one-hot encoding을 해주면(=argmax함수 사용) 0.7만 1의 값이고 0.7을 제외한 결과값은 0이 되어 a로 분류됨을 알 수 있음
23. logits(로짓) = tf.matmul(X,W) + b
- hypothesis = tf.nn.softmax(logits)
- hypothesis : 확률값
24. 다층 퍼셉트론(XOR) : 병렬형식의 단층 네트워크(유닛)를 하나의 네트워크(유닛)으로 결합시킨 후 다음 네트워크(유닛)의 X자리에 대입함
K(X) = sigmoid( X*W1+B1 )
hat(Y) = H(X) = sigmoid( K(X)*W2 + B2 )
=> K = tf.sigmoid(tf.matmul(X,W1) + B1)
hypothesis = tf.sigmoid(tf.matmul(K, W2) + B2)
25. Partial derivative : 관심있는 항목 외엔 상수로 보고 미분하는 방법
26. chain rule : 복합함수 미분 : x가 최종적으로 f에 미치는 영향을 구하기 위함 : f(g(x)) 미분 = df/dx = (df/dg * dg/dx)
27. y 결과값이 0 또는 1일 땐 복잡한 soft_max를 사용할 필요없이 logistic regression을 사용해줌
28. TensorBoard : 학습을 길게 할 때, 학습의 진행사항을 볼 수 있게 해주는 도구
step에 따른 cost 값 변화를 그래프로 보여줌








